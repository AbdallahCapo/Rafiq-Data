{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQLtch4xbCeR",
        "outputId": "7dcdea6e-27ed-4d2f-d279-cb2f7f25f805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `caaapooo` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `caaapooo`\n"
          ]
        }
      ],
      "source": [
        "! huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9e_Y7fYKX0Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers sentence-transformers faiss-cpu accelerate bitsandbytes langdetect\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKQIUp3pcpZk",
        "outputId": "19852c96-4109-42de-b817-fbb4b052ff13"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.3 faiss-cpu-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "import time\n",
        "from langdetect import detect\n",
        "import re"
      ],
      "metadata": {
        "id": "-EsyruX1skOm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    with open(\"/content/DATA_RQ.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "    print(f\"Error loading JSON data: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "\n",
        "qa_pairs = []\n",
        "instructions = []\n",
        "for category, content in data.items():\n",
        "    try:\n",
        "        if isinstance(content, list):\n",
        "            for entry in content:\n",
        "                if isinstance(entry, dict) and \"Q\" in entry and \"A\" in entry:\n",
        "                    answer_text = entry[\"A\"]\n",
        "                    if isinstance(answer_text, list):\n",
        "                        answer_text = \" \".join(answer_text)\n",
        "                    qa_pairs.append((entry[\"Q\"], answer_text))\n",
        "                elif isinstance(entry, dict) and \"question\" in entry and \"answer\" in entry:\n",
        "                    answer_text = entry[\"answer\"]\n",
        "                    if isinstance(answer_text, list):\n",
        "                        answer_text = \" \".join(answer_text)\n",
        "                    qa_pairs.append((entry[\"question\"], answer_text))\n",
        "                elif isinstance(entry, list):\n",
        "                    for sub_entry in entry:\n",
        "                        if isinstance(sub_entry, dict) and \"question\" in sub_entry and \"answer\" in sub_entry:\n",
        "                            answer_text = sub_entry[\"answer\"]\n",
        "                            if isinstance(answer_text, list):\n",
        "                                answer_text = \" \".join(answer_text)\n",
        "                            qa_pairs.append((sub_entry[\"question\"], answer_text))\n",
        "                else:\n",
        "                    instructions.append(str(entry))\n",
        "        elif isinstance(content, dict):\n",
        "            for key, value in content.items():\n",
        "                if isinstance(value, dict) and \"question\" in value and \"answer\" in value:\n",
        "                    answer_text = value[\"answer\"]\n",
        "                    if isinstance(answer_text, list):\n",
        "                        answer_text = \" \".join(answer_text)\n",
        "                    qa_pairs.append((value[\"question\"], answer_text))\n",
        "                elif isinstance(value, list):\n",
        "                    instructions.extend(value)\n",
        "                else:\n",
        "                    instructions.append(str(value))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing data in category {category}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(qa_pairs)} Q&A pairs and {len(instructions)} instructions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toJK9iHXcuGz",
        "outputId": "f3d9d190-5f58-4eba-a053-ff9ceb60736e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3263 Q&A pairs and 502 instructions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading embedding model: {e}\")\n",
        "    exit(1)\n",
        "try:\n",
        "    question_embeddings = embedder.encode([q[0] for q in qa_pairs])\n",
        "    instruction_embeddings = embedder.encode(instructions)\n",
        "except Exception as e:\n",
        "    print(f\"Error generating embeddings: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "try:\n",
        "    dimension = question_embeddings.shape[1]\n",
        "    faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    faiss_index.add(np.array(question_embeddings))\n",
        "    faiss_index.add(np.array(instruction_embeddings))\n",
        "    all_data = qa_pairs + [(inst, inst) for inst in instructions]\n",
        "    print(f\"FAISS index built with {len(all_data)} entries.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error building FAISS index: {e}\")\n",
        "    exit(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGB2zmE4gS_j",
        "outputId": "cf922c86-9e97-4d61-a135-8c1506439a86"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built with 3765 entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/aya-expanse-8b\", timeout=600)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"CohereForAI/aya-expanse-8b\", load_in_4bit=True, device_map=\"auto\"\n",
        "    )\n",
        "    text_gen_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "    print(\"Model and pipeline loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit(1)"
      ],
      "metadata": {
        "id": "WWGJZANEgWvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_conversations = {}\n",
        "\n",
        "def get_dynamic_word_range(user_input, retrieved_answers):\n",
        "    \"\"\"Determine the word count range dynamically for response generation.\"\"\"\n",
        "    try:\n",
        "        language = detect(user_input)\n",
        "    except:\n",
        "        language = \"en\"\n",
        "\n",
        "    num_retrieved = sum(len(ans.split()) for ans in retrieved_answers) if retrieved_answers else 0\n",
        "    num_input_words = len(user_input.split())\n",
        "    avg_len = num_retrieved + num_input_words\n",
        "\n",
        "    language_factors = {\n",
        "        \"en\": 1.0, \"ar\": 0.8, \"fr\": 1.1, \"zh\": 0.5, \"es\": 1.05, \"de\": 1.0,\n",
        "        \"ru\": 1.1, \"it\": 1.0, \"pt\": 1.05, \"ja\": 0.6, \"ko\": 0.7, \"tr\": 0.9\n",
        "    }\n",
        "    factor = language_factors.get(language, 1.0)\n",
        "\n",
        "    base_max_len = int((avg_len + 30) * factor)\n",
        "    max_chars = min(600, base_max_len + 200)\n",
        "    min_chars = max(200, base_max_len - 100)\n",
        "\n",
        "    return max(150, min_chars), max(500, max_chars)"
      ],
      "metadata": {
        "id": "095PLNXc-THZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_response(text):\n",
        "    \"\"\"Format response to improve readability.\"\"\"\n",
        "    text = re.sub(r\"<\\|im_end\\|>|<\\|endoftext\\|>\", \"\", text, flags=re.IGNORECASE).strip()\n",
        "    text = re.sub(r\"\\(\\d+\\s*(characters|chars)\\)\", \"\", text).strip()\n",
        "\n",
        "    formatted_text = []\n",
        "    paragraphs = text.split(\"\\n\")\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        words = paragraph.split()\n",
        "        line = \"\"\n",
        "        for word in words:\n",
        "            if len(line) + len(word) > 90:\n",
        "                formatted_text.append(line.strip())\n",
        "                line = \"\"\n",
        "            line += f\"{word} \"\n",
        "        if line:\n",
        "            formatted_text.append(line.strip())\n",
        "\n",
        "    return \"\\n\".join(formatted_text)"
      ],
      "metadata": {
        "id": "n7Kad4FlBPWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_bullets(text):\n",
        "    \"\"\"Convert lists into bullet points where applicable.\"\"\"\n",
        "    lines = text.split(\"\\n\")\n",
        "    formatted_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if re.match(r\"^\\d+\\.\", line) or re.match(r\"^-\", line):\n",
        "            formatted_lines.append(f\"- {line.lstrip('-').lstrip()}\")\n",
        "        else:\n",
        "            formatted_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(formatted_lines)"
      ],
      "metadata": {
        "id": "fPELgri0BRgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_conclusion(text):\n",
        "    \"\"\"Remove unnecessary conclusion phrases.\"\"\"\n",
        "    unwanted_phrases = [\"Conclusion:\", \"Summary:\", \"Final Thoughts:\"]\n",
        "    lines = text.split(\"\\n\")\n",
        "\n",
        "    if lines:\n",
        "        last_line = lines[-1]\n",
        "        for phrase in unwanted_phrases:\n",
        "            if last_line.startswith(phrase):\n",
        "                last_line = last_line[len(phrase):].strip()\n",
        "                break\n",
        "        lines[-1] = last_line\n",
        "\n",
        "    return \"\\n\".join(lines)"
      ],
      "metadata": {
        "id": "FUptpE18BSqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_answer(user_question, top_k=3, threshold=0.7):\n",
        "    \"\"\"Retrieve the most relevant answers from FAISS index.\"\"\"\n",
        "    try:\n",
        "        user_embedding = embedder.encode([user_question])\n",
        "        distances, indices = faiss_index.search(np.array(user_embedding), top_k)\n",
        "\n",
        "        results = []\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            if dist < threshold and 0 <= idx < len(all_data):\n",
        "                results.append(all_data[idx][1])\n",
        "\n",
        "        return results if results else []\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving answer: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "shRjB96KBTuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(user_id, user_input):\n",
        "    \"\"\"Generate chatbot responses using chat history, retrieved knowledge, and AI reasoning.\"\"\"\n",
        "    global user_conversations\n",
        "\n",
        "    if user_id not in user_conversations:\n",
        "        user_conversations[user_id] = []\n",
        "\n",
        "    user_conversations[user_id].append(f\"User: {user_input}\")\n",
        "\n",
        "    retrieved_answers = retrieve_answer(user_input)\n",
        "\n",
        "\n",
        "    if retrieved_answers and len(retrieved_answers[0].split()) < 50:\n",
        "        response_text = retrieved_answers[0]\n",
        "        user_conversations[user_id].append(f\"Chatbot: {response_text}\")\n",
        "        return response_text\n",
        "\n",
        "\n",
        "    context = \" \".join(retrieved_answers[:2]) if retrieved_answers else None\n",
        "\n",
        "    min_chars, max_chars = get_dynamic_word_range(user_input, retrieved_answers)\n",
        "\n",
        "\n",
        "    conversation_history = \"\\n\".join(user_conversations[user_id][-5:])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are an AI assistant specialized in answering questions.\n",
        "    - Use provided context when available.\n",
        "    - Ensure responses are **detailed but concise**.\n",
        "    - Use **bullet points** when listing details.\n",
        "    - Do not repeat information unnecessarily.\n",
        "    - Maintain a **conversational tone** for follow-up questions.\n",
        "\n",
        "    {f'Context: {context}' if context else 'No specific context available.'}\n",
        "\n",
        "    Conversation history:\n",
        "    {conversation_history}\n",
        "\n",
        "    User question: {user_input}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = text_gen_pipeline(\n",
        "            prompt,\n",
        "            max_length=max_chars,\n",
        "            min_length=min_chars,\n",
        "            do_sample=True,\n",
        "            top_p=0.85,\n",
        "            temperature=0.7,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        generated_text = response[0]['generated_text']\n",
        "        response_text = generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "        response_text = format_response(response_text)\n",
        "        response_text = add_bullets(response_text)\n",
        "        response_text = clear_conclusion(response_text)\n",
        "\n",
        "        if len(response_text) < min_chars:\n",
        "            response_text += \"\\n- Let me know if you need more details.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        response_text = \"Sorry, I can't generate a response at the moment. Please try again later.\"\n",
        "\n",
        "    if response_text and response_text[-1] not in \".!?\":\n",
        "        response_text += \".\"\n",
        "\n",
        "    user_conversations[user_id].append(f\"Chatbot: {response_text}\")\n",
        "\n",
        "    return response_text\n"
      ],
      "metadata": {
        "id": "OF3FHOs7-TLq"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_id = \"user_123\"\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"You: \").strip()\n",
        "        if not user_input:\n",
        "            print(\"Chatbot: Please enter a valid message.\")\n",
        "            continue\n",
        "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        start_time = time.time()\n",
        "        response = generate_response(user_id, user_input)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        if elapsed_time > 60:\n",
        "            print(\"Chatbot: Sorry for the delay. Here’s your response:\")\n",
        "\n",
        "        print(f\"Chatbot: {response}\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nChatbot: Session ended. Goodbye!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error: {e}\")"
      ],
      "metadata": {
        "id": "KuAe22v-VQmo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}